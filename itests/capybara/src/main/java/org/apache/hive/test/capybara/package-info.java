/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Capybara is a integration test framework for Hive.  It has several goals:
 * <ul>
 * <li>Separate writing and running tests from permutations of features in Hive.  That is, one
 * test should be able to run in beeline, CLI, and JDBC, with ORC, Parquet, RCFile, etc., with
 * Tez or Spark as an engine, etc.</li>
 * <li>The same tests should run locally on developer machines and against a cluster.  These
 * tests should be able to automatically scale as appropriate to their location.  This allows
 * simple testing for devs on their boxes with a few K of data and for users on their clusters
 * with G or T of data.</li>
 * <li>Expected results of the queries should be auto-generated by the system.  It's insane to
 * rely on dev's eyeballing results.</li>
 * <li>Access to the query plan should be provided in code, rather than requiring string compares
 * to see if the output of explain looks the same today as it did yesterday.</li>
 * <li>The tool should leverage JUnit and Maven.  It should be in Java so that developers can
 * test a variety of scenarios beyond just SQL queries.</li>
 * <li>The tool should be able to simulate user data.  Given users queries and access to the
 * user's tables, it should generate data like the user data and then build tests for the user's
 * queries.</li>
 *
 * </ul>
 *
 * <p>Capybara works by managing an instance of Hive and a Benchmark.  When running locally the
 * Hive instance will be run in process using a DFSMiniCluster (and other mini-clusters as
 * appropriate).  When running on a cluster the Hive instance will connect to Hive via CLI or
 * JDBC (depending on how the system is configured).  The Benchmark usually uses a RDBMS (by
 * default Derby locally and Postgres on the cluster) to run the same query and compare the
 * results.  To some extent the system can smooth over the differences between Hive SQL and ANSI
 * standard SQL.  If you need to run a completely different query against the benchmark (say
 * you're testing a UDF not supported in the Benchmark) then you can run separate queries against
 * Hive and the Benchmark.  For extreme cases you can also provide your own Benchmark
 * implementation.
 * </p>
 *
 * <p>To use the framework create a JUnit4 test (i.e. one that uses @Test) and have it extend
 * {@link org.apache.hive.test.capybara.IntegrationTest}.  IntegrationTest handles setup and
 * teardown of cluster and benchmark resources.</p>
 *
 * <p>{@link org.apache.hive.test.capybara.TableTool}
 * provides methods to build a number of commonly used test tables.  You can also use
 * {@link org.apache.hive.test.capybara.infra.TestTable} to build your own table and populate it.
 * The system keeps track of tables created and populated in both Hive and the Benchmark and will
 * not re-create the tables if they already exist.  It will detect changes in scale, file format,
 * etc. that will require a re-creation and then handle dropping and re-creating the table.</p>
 *
 * <p>Once your tables are created and populated you can run queries against them using
 * {@link org.apache.hive.test.capybara.IntegrationTest#runQuery(String)} and related methods.
 * You can run any number of queries desired in the test.  All operations will be run against
 * both Hive and the Benchmark.  If you need to set any configuartion values, this should be done
 * via calls to {@link org.apache.hive.test.capybara.IntegrationTest#set}.  The configuration is
 * reset for each test so that set calls from one test do not affect any other tests.  If you
 * have a set of values you would like to have set for all tests in a file you can do that in a
 * &commat;Before method.
 * </p>
 *
 * <p>Some features require a set of configurtion values to be set.  Rather than requiring test
 * writers to set these up each time, annotations are provided that will tell the system to set
 * the appropriate configuration values for a test.  For example, all of the values for testing
 * SQL Standard Authorization can be turned on by annotating a test with @SqlStdAuthOn.  See the
 * {@link org.apache.hive.test.capybara.annotations} package for a full list.</p>
 *
 * <p>Once you have produced a result you would like to compare you can use one of the
 * comparison functions to check your results.  For select queries
 * {@link org.apache.hive.test.capybara.IntegrationTest#compare} will compare results in the
 * order they are returned.  This should only be used if you expect your data to be sorted.
 * {@link org.apache.hive.test.capybara.IntegrationTest#sortAndCompare} will sort results and
 * compare them.  {@link org.apache.hive.test.capybara.IntegrationTest#tableCompare} will
 * compare entries in two tables.  This is useful for insert queries.</p>
 *
 * <p>Which features are being tested (e.g. which file format is used for tables, which
 * execution engine, how Hive is accessed, etc.) are controlled by system properties.  You can
 * find the system properties to set as well as default values by looking in
 * {@link org.apache.hive.test.capybara.infra.TestConf}.  To change these values you pass
 * properties to JUnit as part of your maven build command.  For example, to use Tez as your
 * execution engine instead of the default (currently unspecified, which means Hive's local mode
 * on your machine and the cluster default on yoru cluster), you would give a command like:
 * <tt>mvn test -Dtest=ExampleTest -Dhive.test.capybara.engine=tez</tt>
 * </p>
 *
 * <p>When running on a cluster you must tell capybara explicitly that it is on a cluster, and
 * where to find configuration information for that cluster.  The system property to tell it to
 * run on the cluster is <tt>hive.test.capybara.use.cluster</tt>.  To tell it where to find
 * Hadoop you need to define the property <tt>HADOOP_HOME</tt>.  To find Hive you need to set
 * the property <tt>HIVE_HOME</tt>.  If your postgres database is password protected, you can
 * pass that via the property <tt>hive.test.capybara.postgres.password</tt>.  So to run
 * ExampleTest on a cluster the command is
 * <tt>mvn test -Dtest=ExampleTest -Dhive.test.capybara.postgres.password=</tt><i>yourpasswd</i><tt>
 * -Dhive.test.capybara.use.cluster=true -DHADOOP_HOME=</tt><i>hadoop_location</i><tt>
 * -DHIVE_HOME=</tt><i>hive_location</i>
 * </p>
 *
 * <p>Some tests do not make sense in some contexts.  For example, currently ACID features are
 * only supported when using ORC file format.  Therefore tests making use of ACID features
 * should not be run when the file format being tested is anything other than ORC.  To control
 * this you can annotate your tests to indicate when they should not be run.  In the ACID case
 * you would mark it @NoParquet, @NoTextFile, @NoRcFile.  You can see the complete list of
 * annotations in {@link org.apache.hive.test.capybara.annotations}.</p>
 *
 */
package org.apache.hive.test.capybara;